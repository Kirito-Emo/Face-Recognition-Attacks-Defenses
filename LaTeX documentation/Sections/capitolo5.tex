\chapter{Considerazioni finali e sviluppi futuri}
    Lo studio condotto ha permesso di valutare la robustezza di un sistema di riconoscimento facciale pre-addestrato rispetto a diversi attacchi avversari e di testare una tecnica difensiva per mitigarne gli effetti. I risultati ottenuti confermano l’efficacia delle perturbazioni generate con metodi noti e mostrano come anche modifiche impercettibili all’occhio umano possano compromettere drasticamente le prestazioni del modello.
    Tuttavia, il lavoro è stato svolto all’interno di un contesto con risorse limitate, sia in termini di tempo computazionale che di disponibilità hardware. Questo ha imposto alcune scelte progettuali conservative, come la generazione di un numero contenuto di esempi avversari e un numero limitato di iterazioni per gli attacchi più onerosi, come \textit{Carlini Wagner} o \textit{DeepFool}. \\
    In un contesto ideale, privo di tali vincoli e con l’accesso a maggiori risorse computazionali (ad esempio più GPU e memoria disponibile), sarebbe possibile estendere l’esperimento a un’analisi più approfondita e sistematica.
    In particolare, si potrebbero aumentare sensibilmente il numero di campioni avversari generati e le iterazioni degli algoritmi di attacco, migliorando così l’analisi quantitativa e qualitativa degli effetti prodotti sul classificatore.
    Un altro aspetto critico emerso riguarda il \textbf{meccanismo di rilevamento degli adversarial examples}: sebbene esso non produca falsi positivi, si è osservata una significativa percentuale di \textit{falsi negativi}, ovvero campioni avversari che non vengono riconosciuti come tali. Questo comportamento limita l’affidabilità complessiva del sistema. Un possibile sviluppo futuro potrebbe quindi riguardare l’ottimizzazione del detector, integrando tecniche più avanzate come analisi delle attivazioni interne, classificatori addestrati ad hoc o approcci basati su \textit{ensemble}.